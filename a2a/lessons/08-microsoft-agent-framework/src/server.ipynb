{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44857417",
   "metadata": {},
   "source": [
    "# Lesson 08 — Microsoft Agent Framework: A2A Server\n",
    "\n",
    "This notebook shows how to build an **A2A server** using **Microsoft Agent Framework** using your choice of model provider.\n",
    "\n",
    "## What we build\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    LLM[\"AzureOpenAIChatClient\\nGitHub Phi-4 or LocalFoundry\"]\n",
    "    LLM -->|\".as_agent()\"| Agent[\"Agent\\nname · instructions · tools\"]\n",
    "    Agent --> Exec[\"PolicyQAExecutor\\nAgentExecutor\"]\n",
    "    Exec --> Handler[\"DefaultRequestHandler\\nInMemoryTaskStore\"]\n",
    "    Handler --> Server[\"A2AStarletteApplication\\nport 10081\"]\n",
    "    Server -->|\"A2A JSON-RPC\"| Client[\"Any A2A Client\"]\n",
    "```\n",
    "\n",
    "**Same use case as Lesson 05–07:** insurance policy Q&A assistant.\n",
    "\n",
    "### Model provider options\n",
    "\n",
    "| Provider                    | Variable value   | What you need                                                                 |\n",
    "| --------------------------- | ---------------- | ----------------------------------------------------------------------------- |\n",
    "| **GitHub Models**           | `\"github\"`       | `GITHUB_TOKEN` in `.env` — [get one free](https://github.com/settings/tokens) |\n",
    "| **AI Toolkit LocalFoundry** | `\"localfoundry\"` | VS Code AI Toolkit extension, model loaded on port 5272                       |\n",
    "\n",
    "Set `PROVIDER` in cell 2 before running.\n",
    "\n",
    "Port used: **10081** (notebooks) · Full lesson script uses 10008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957d8294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Provider : github\n",
      "✓ Endpoint : https://models.inference.ai.azure.com\n",
      "✓ Model    : Phi-4\n",
      "✓ Port     : 10081\n"
     ]
    }
   ],
   "source": [
    "# ── Imports & environment setup ──────────────────────────────────\n",
    "import os\n",
    "import threading\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(raise_error_if_not_found=False))\n",
    "\n",
    "# ── Model provider ────────────────────────────────────────────────\n",
    "# \"github\"       — GitHub Models  (free, needs GITHUB_TOKEN in .env)\n",
    "#                  https://github.com/settings/tokens\n",
    "# \"localfoundry\" — AI Toolkit LocalFoundry  (local, no token needed)\n",
    "#                  VS Code AI Toolkit → Models → Load a model → Run\n",
    "PROVIDER = \"github\"  # ← change to \"localfoundry\" to use a local model\n",
    "\n",
    "if PROVIDER == \"github\":\n",
    "    ENDPOINT = \"https://models.inference.ai.azure.com\"\n",
    "    API_KEY = os.environ.get(\"GITHUB_TOKEN\", \"\")\n",
    "    MODEL = \"Phi-4\"\n",
    "    if not API_KEY:\n",
    "        raise EnvironmentError(\"Set GITHUB_TOKEN in your .env or environment first\")\n",
    "\n",
    "elif PROVIDER == \"localfoundry\":\n",
    "    ENDPOINT = \"http://localhost:5272/v1/\"\n",
    "    API_KEY = \"unused\"  # LocalFoundry ignores the key\n",
    "    MODEL = \"Phi-4-mini-instruct\"  # ← change to your loaded model ID\n",
    "    print(\"ℹ  AI Toolkit LocalFoundry — ensure a model is running on port 5272\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown PROVIDER: {PROVIDER!r}\")\n",
    "\n",
    "SERVER_PORT = 10081\n",
    "\n",
    "print(f\"✓ Provider : {PROVIDER}\")\n",
    "print(f\"✓ Endpoint : {ENDPOINT}\")\n",
    "print(f\"✓ Model    : {MODEL}\")\n",
    "print(f\"✓ Port     : {SERVER_PORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf2ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy document loaded.\n",
      "  Length: 911 characters\n"
     ]
    }
   ],
   "source": [
    "# ── Knowledge base (inline — no file path needed) ────────────────\n",
    "POLICY = \"\"\"\n",
    "BRIGHT SHIELD INSURANCE — POLICY SUMMARY\n",
    "\n",
    "Coverage Period:  January 1 – December 31 2025\n",
    "Policy Number:    BS-2025-DEMO\n",
    "\n",
    "DEDUCTIBLES\n",
    "  Annual deductible (individual) : $500\n",
    "  Annual deductible (family)     : $1,000\n",
    "  Out-of-pocket maximum          : $3,500 / person\n",
    "\n",
    "PREMIUMS\n",
    "  Monthly premium (individual)   : $120\n",
    "  Monthly premium (family)       : $340\n",
    "\n",
    "COVERAGE\n",
    "  Liability                      : $100,000 per incident\n",
    "  Collision                      : Covered (ACV after deductible)\n",
    "  Comprehensive                  : Covered (ACV after deductible)\n",
    "  Rental car reimbursement       : Up to $35/day, 30-day maximum\n",
    "  Emergency roadside assistance  : Included (unlimited calls)\n",
    "  Medical payments               : $5,000 per person\n",
    "\n",
    "EXCLUSIONS\n",
    "  Intentional damage, racing, commercial use are NOT covered.\n",
    "\n",
    "CLAIMS\n",
    "  File within 30 days of incident.\n",
    "  Contact: claims@brightshield.example  |  1-800-555-0199\n",
    "\"\"\"\n",
    "\n",
    "print(\"Policy document loaded.\")\n",
    "print(f\"  Length: {len(POLICY)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0abcc79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Agent created : PolicyQAAgent\n",
      "  Provider      : github  (Phi-4)\n"
     ]
    }
   ],
   "source": [
    "# ── Build the Microsoft Agent Framework agent ─────────────────────\n",
    "#\n",
    "# Pattern:\n",
    "#   1. AzureOpenAIChatClient  — LLM connection (endpoint set by PROVIDER)\n",
    "#   2. .as_agent()            — wraps client with name + instructions\n",
    "#   3. await agent.run(msg)   — call the LLM\n",
    "\n",
    "from agent_framework.azure import AzureOpenAIChatClient  # type: ignore[attr-defined]\n",
    "\n",
    "INSTRUCTIONS = f\"\"\"\\\n",
    "You are a helpful insurance policy assistant for Bright Shield Insurance.\n",
    "Answer questions accurately using the policy document provided.\n",
    "If the answer is not in the document, say so clearly.\n",
    "Keep answers concise — two or three sentences.\n",
    "\n",
    "--- POLICY DOCUMENT ---\n",
    "{POLICY}\n",
    "--- END ---\n",
    "\"\"\"\n",
    "\n",
    "chat_client = AzureOpenAIChatClient(\n",
    "    api_key=API_KEY,\n",
    "    endpoint=ENDPOINT,\n",
    "    deployment_name=MODEL,\n",
    ")\n",
    "\n",
    "agent = chat_client.as_agent(\n",
    "    name=\"PolicyQAAgent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    tools=[],  # no tools — pure LLM Q&A\n",
    ")\n",
    "\n",
    "print(f\"✓ Agent created : {agent.name}\")\n",
    "print(f\"  Provider      : {PROVIDER}  ({MODEL})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14626422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python311\\Lib\\ast.py:50: RuntimeWarning: coroutine 'ask' was never awaited\n",
      "  return compile(source, filename, mode, flags,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "ChatClientException",
     "evalue": "<class 'agent_framework.azure._chat_client.AzureOpenAIChatClient'> service failed to complete the prompt: NOT FOUND",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32my:\\.sources\\localm-tuts\\a2a\\_examples\\a2a\\.venv\\Lib\\site-packages\\agent_framework\\openai\\_chat_client.py:262\u001b[39m, in \u001b[36mRawOpenAIChatClient._inner_get_response.<locals>._get_response\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_response_from_openai(\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(stream=\u001b[38;5;28;01mFalse\u001b[39;00m, **options_dict), options\n\u001b[32m    263\u001b[39m     )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequestError \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32my:\\.sources\\localm-tuts\\a2a\\_examples\\a2a\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2700\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2699\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2700\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2701\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2702\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2703\u001b[39m         {\n\u001b[32m   2704\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2705\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2706\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2707\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2708\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2709\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2710\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2711\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2712\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2713\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2714\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2715\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2716\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2717\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2718\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2719\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2720\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2721\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2722\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2723\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2724\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2725\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2726\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2727\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2728\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2729\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2730\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2731\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2732\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2733\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2734\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2735\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2736\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2737\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2738\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2739\u001b[39m         },\n\u001b[32m   2740\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2741\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2742\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2743\u001b[39m     ),\n\u001b[32m   2744\u001b[39m     options=make_request_options(\n\u001b[32m   2745\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2746\u001b[39m     ),\n\u001b[32m   2747\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2748\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2749\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2750\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32my:\\.sources\\localm-tuts\\a2a\\_examples\\a2a\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1884\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1881\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1882\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1883\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1884\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32my:\\.sources\\localm-tuts\\a2a\\_examples\\a2a\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1669\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1668\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1669\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1671\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: NOT FOUND",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatClientException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ── Quick smoke-test: call the agent directly ─────────────────────\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Jupyter already runs inside an event loop — use `await` directly\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# rather than asyncio.run(), which raises RuntimeError in a notebook.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m answer = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(\u001b[33m\"\u001b[39m\u001b[33mWhat is the annual deductible for an individual?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQ: What is the annual deductible for an individual?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32my:\\.sources\\localm-tuts\\a2a\\_examples\\a2a\\.venv\\Lib\\site-packages\\agent_framework\\_agents.py:838\u001b[39m, in \u001b[36mRawAgent.run.<locals>._run_non_streaming\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    830\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_non_streaming\u001b[39m() -> AgentResponse[Any]:\n\u001b[32m    831\u001b[39m     ctx = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prepare_run_context(\n\u001b[32m    832\u001b[39m         messages=messages,\n\u001b[32m    833\u001b[39m         session=session,\n\u001b[32m   (...)\u001b[39m\u001b[32m    836\u001b[39m         kwargs=kwargs,\n\u001b[32m    837\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.get_response(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m    839\u001b[39m         messages=ctx[\u001b[33m\"\u001b[39m\u001b[33msession_messages\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    840\u001b[39m         stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    841\u001b[39m         options=ctx[\u001b[33m\"\u001b[39m\u001b[33mchat_options\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    842\u001b[39m         **ctx[\u001b[33m\"\u001b[39m\u001b[33mfiltered_kwargs\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    843\u001b[39m     )\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response:\n\u001b[32m    846\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m AgentInvalidResponseException(\u001b[33m\"\u001b[39m\u001b[33mChat client did not return a response.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32my:\\.sources\\localm-tuts\\a2a\\_examples\\a2a\\.venv\\Lib\\site-packages\\agent_framework\\_tools.py:2139\u001b[39m, in \u001b[36mFunctionInvocationLayer.get_response.<locals>._get_response\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2136\u001b[39m errors_in_a_row = approval_result[\u001b[33m\"\u001b[39m\u001b[33merrors_in_a_row\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2137\u001b[39m total_function_calls += approval_result.get(\u001b[33m\"\u001b[39m\u001b[33mfunction_call_count\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2139\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m super_get_response(\n\u001b[32m   2140\u001b[39m     messages=prepped_messages,\n\u001b[32m   2141\u001b[39m     stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2142\u001b[39m     options=mutable_options,\n\u001b[32m   2143\u001b[39m     **filtered_kwargs,\n\u001b[32m   2144\u001b[39m )\n\u001b[32m   2146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.conversation_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2147\u001b[39m     _update_conversation_id(kwargs, response.conversation_id, mutable_options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32my:\\.sources\\localm-tuts\\a2a\\_examples\\a2a\\.venv\\Lib\\site-packages\\agent_framework\\openai\\_chat_client.py:275\u001b[39m, in \u001b[36mRawOpenAIChatClient._inner_get_response.<locals>._get_response\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatClientException(\n\u001b[32m    271\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m         inner_exception=ex,\n\u001b[32m    273\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatClientException(\n\u001b[32m    276\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m         inner_exception=ex,\n\u001b[32m    278\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mChatClientException\u001b[39m: <class 'agent_framework.azure._chat_client.AzureOpenAIChatClient'> service failed to complete the prompt: NOT FOUND"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ── Quick smoke-test: call the agent directly ─────────────────────\n",
    "#\n",
    "# Jupyter already runs inside an event loop — use `await` directly\n",
    "# rather than asyncio.run(), which raises RuntimeError in a notebook.\n",
    "\n",
    "answer = await agent.run(\"What is the annual deductible for an individual?\")\n",
    "print(\"Q: What is the annual deductible for an individual?\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ebb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Wrap the agent as an A2A server ──────────────────────────────\n",
    "#\n",
    "# Steps (same as Lesson 06 but using MAF instead of QAAgent):\n",
    "#   1. Implement AgentExecutor.execute()  — calls agent.run()\n",
    "#   2. Build AgentCard                    — describes the agent\n",
    "#   3. Wire DefaultRequestHandler         — task lifecycle management\n",
    "#   4. Build A2AStarletteApplication      — ASGI server\n",
    "\n",
    "from a2a.server.agent_execution import AgentExecutor, RequestContext\n",
    "from a2a.server.apps import A2AStarletteApplication\n",
    "from a2a.server.events import EventQueue\n",
    "from a2a.server.request_handlers import DefaultRequestHandler\n",
    "from a2a.server.tasks import InMemoryTaskStore\n",
    "from a2a.types import AgentCapabilities, AgentCard, AgentSkill\n",
    "from a2a.utils import new_agent_text_message\n",
    "from pydantic import AnyHttpUrl\n",
    "\n",
    "\n",
    "class PolicyQAExecutor(AgentExecutor):\n",
    "    \"\"\"Bridges the A2A protocol to the MAF agent.\"\"\"\n",
    "\n",
    "    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:\n",
    "        question = context.get_user_input()\n",
    "        answer = await agent.run(question)\n",
    "        await event_queue.enqueue_event(new_agent_text_message(str(answer)))\n",
    "\n",
    "    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"PolicyQAAgent\",\n",
    "    description=\"Insurance policy Q&A — Microsoft Agent Framework + GitHub Phi-4\",\n",
    "    url=AnyHttpUrl(f\"http://localhost:{SERVER_PORT}/\"),\n",
    "    version=\"1.0.0\",\n",
    "    capabilities=AgentCapabilities(streaming=False),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"policy_qa\",\n",
    "            name=\"Policy Q&A\",\n",
    "            description=\"Answer questions about the Bright Shield insurance policy\",\n",
    "            tags=[\"insurance\", \"qa\"],\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "handler = DefaultRequestHandler(\n",
    "    agent_executor=PolicyQAExecutor(),\n",
    "    task_store=InMemoryTaskStore(),\n",
    ")\n",
    "\n",
    "server_app = A2AStarletteApplication(agent_card=agent_card, http_handler=handler)\n",
    "app = server_app.build()\n",
    "\n",
    "print(\"✓ A2A app built\")\n",
    "print(f\"  Agent card : http://localhost:{SERVER_PORT}/.well-known/agent-card.json\")\n",
    "print(f\"  JSON-RPC   : POST http://localhost:{SERVER_PORT}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Start the server in a background thread ──────────────────────\n",
    "#\n",
    "# The server runs in a daemon thread so the notebook stays interactive.\n",
    "# Open client.ipynb to send A2A requests to this server.\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "\n",
    "def _run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=SERVER_PORT, log_level=\"warning\")\n",
    "\n",
    "\n",
    "server_thread = threading.Thread(target=_run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "import time\n",
    "\n",
    "time.sleep(1.0)  # give uvicorn a moment to bind\n",
    "\n",
    "import httpx\n",
    "\n",
    "resp = httpx.get(\n",
    "    f\"http://localhost:{SERVER_PORT}/.well-known/agent-card.json\", timeout=5\n",
    ")\n",
    "card = resp.json()\n",
    "print(f\"✓ Server live — status {resp.status_code}\")\n",
    "print(f\"  Name        : {card.get('name')}\")\n",
    "print(f\"  Description : {card.get('description')}\")\n",
    "print()\n",
    "print(\"Now open client.ipynb to send A2A requests →\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
