{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44857417",
   "metadata": {},
   "source": [
    "# Lesson 08 — Microsoft Agent Framework: A2A Server\n",
    "\n",
    "This notebook shows how to build an **A2A server** using **Microsoft Agent Framework** using your choice of model provider.\n",
    "\n",
    "## What we build\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    LLM[\"AzureOpenAIChatClient\\nGitHub Phi-4 or LocalFoundry\"]\n",
    "    LLM -->|\".as_agent()\"| Agent[\"Agent\\nname · instructions · tools\"]\n",
    "    Agent --> Exec[\"PolicyQAExecutor\\nAgentExecutor\"]\n",
    "    Exec --> Handler[\"DefaultRequestHandler\\nInMemoryTaskStore\"]\n",
    "    Handler --> Server[\"A2AStarletteApplication\\nport 10081\"]\n",
    "    Server -->|\"A2A JSON-RPC\"| Client[\"Any A2A Client\"]\n",
    "```\n",
    "\n",
    "**Same use case as Lesson 05–07:** insurance policy Q&A assistant.\n",
    "\n",
    "### Model provider options\n",
    "\n",
    "| Provider                    | Variable value   | What you need                                                                 |\n",
    "| --------------------------- | ---------------- | ----------------------------------------------------------------------------- |\n",
    "| **GitHub Models**           | `\"github\"`       | `GITHUB_TOKEN` in `.env` — [get one free](https://github.com/settings/tokens) |\n",
    "| **AI Toolkit LocalFoundry** | `\"localfoundry\"` | VS Code AI Toolkit extension, model loaded on port 5272                       |\n",
    "\n",
    "Set `PROVIDER` in cell 2 before running.\n",
    "\n",
    "Port used: **10081** (notebooks) · Full lesson script uses 10008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957d8294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Provider : github\n",
      "✓ Endpoint : https://models.inference.ai.azure.com\n",
      "✓ Model    : Phi-4\n",
      "✓ Port     : 10081\n"
     ]
    }
   ],
   "source": [
    "# ── Imports & environment setup ──────────────────────────────────\n",
    "import os\n",
    "import threading\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(raise_error_if_not_found=False))\n",
    "\n",
    "# ── Model provider ────────────────────────────────────────────────\n",
    "# \"github\"       — GitHub Models  (free, needs GITHUB_TOKEN in .env)\n",
    "#                  https://github.com/settings/tokens\n",
    "# \"localfoundry\" — AI Toolkit LocalFoundry  (local, no token needed)\n",
    "#                  VS Code AI Toolkit → Models → Load a model → Run\n",
    "PROVIDER = \"github\"  # ← change to \"localfoundry\" to use a local model\n",
    "\n",
    "if PROVIDER == \"github\":\n",
    "    ENDPOINT = \"https://models.inference.ai.azure.com\"\n",
    "    API_KEY = os.environ.get(\"GITHUB_TOKEN\", \"\")\n",
    "    MODEL = \"Phi-4\"\n",
    "    if not API_KEY:\n",
    "        raise EnvironmentError(\"Set GITHUB_TOKEN in your .env or environment first\")\n",
    "\n",
    "elif PROVIDER == \"localfoundry\":\n",
    "    ENDPOINT = \"http://localhost:5272/v1/\"\n",
    "    API_KEY = \"unused\"  # LocalFoundry ignores the key\n",
    "    MODEL = \"Phi-4-mini-instruct\"  # ← change to your loaded model ID\n",
    "    print(\"ℹ  AI Toolkit LocalFoundry — ensure a model is running on port 5272\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown PROVIDER: {PROVIDER!r}\")\n",
    "\n",
    "SERVER_PORT = 10081\n",
    "\n",
    "print(f\"✓ Provider : {PROVIDER}\")\n",
    "print(f\"✓ Endpoint : {ENDPOINT}\")\n",
    "print(f\"✓ Model    : {MODEL}\")\n",
    "print(f\"✓ Port     : {SERVER_PORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf2ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy document loaded.\n",
      "  Length: 911 characters\n"
     ]
    }
   ],
   "source": [
    "# ── Knowledge base (inline — no file path needed) ────────────────\n",
    "POLICY = \"\"\"\n",
    "BRIGHT SHIELD INSURANCE — POLICY SUMMARY\n",
    "\n",
    "Coverage Period:  January 1 – December 31 2025\n",
    "Policy Number:    BS-2025-DEMO\n",
    "\n",
    "DEDUCTIBLES\n",
    "  Annual deductible (individual) : $500\n",
    "  Annual deductible (family)     : $1,000\n",
    "  Out-of-pocket maximum          : $3,500 / person\n",
    "\n",
    "PREMIUMS\n",
    "  Monthly premium (individual)   : $120\n",
    "  Monthly premium (family)       : $340\n",
    "\n",
    "COVERAGE\n",
    "  Liability                      : $100,000 per incident\n",
    "  Collision                      : Covered (ACV after deductible)\n",
    "  Comprehensive                  : Covered (ACV after deductible)\n",
    "  Rental car reimbursement       : Up to $35/day, 30-day maximum\n",
    "  Emergency roadside assistance  : Included (unlimited calls)\n",
    "  Medical payments               : $5,000 per person\n",
    "\n",
    "EXCLUSIONS\n",
    "  Intentional damage, racing, commercial use are NOT covered.\n",
    "\n",
    "CLAIMS\n",
    "  File within 30 days of incident.\n",
    "  Contact: claims@brightshield.example  |  1-800-555-0199\n",
    "\"\"\"\n",
    "\n",
    "print(\"Policy document loaded.\")\n",
    "print(f\"  Length: {len(POLICY)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abcc79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Agent created : PolicyQAAgent\n",
      "  Provider      : github  (Phi-4)\n"
     ]
    }
   ],
   "source": [
    "# ── Build the Microsoft Agent Framework agent ─────────────────────\n",
    "#\n",
    "# Pattern:\n",
    "#   1. AzureOpenAIChatClient  — LLM connection (endpoint set by PROVIDER)\n",
    "#   2. .as_agent()            — wraps client with name + instructions\n",
    "#   3. await agent.run(msg)   — call the LLM\n",
    "\n",
    "from agent_framework.azure import AzureOpenAIChatClient  # type: ignore[attr-defined]\n",
    "\n",
    "INSTRUCTIONS = f\"\"\"\\\n",
    "You are a helpful insurance policy assistant for Bright Shield Insurance.\n",
    "Answer questions accurately using the policy document provided.\n",
    "If the answer is not in the document, say so clearly.\n",
    "Keep answers concise — two or three sentences.\n",
    "\n",
    "--- POLICY DOCUMENT ---\n",
    "{POLICY}\n",
    "--- END ---\n",
    "\"\"\"\n",
    "\n",
    "chat_client = AzureOpenAIChatClient(\n",
    "    api_key=API_KEY,\n",
    "    endpoint=ENDPOINT,\n",
    "    deployment_name=MODEL,\n",
    ")\n",
    "\n",
    "agent = chat_client.as_agent(\n",
    "    name=\"PolicyQAAgent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    tools=[],  # no tools — pure LLM Q&A\n",
    ")\n",
    "\n",
    "print(f\"✓ Agent created : {agent.name}\")\n",
    "print(f\"  Provider      : {PROVIDER}  ({MODEL})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14626422",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(question)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(result)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m answer = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the annual deductible for an individual?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQ: What is the annual deductible for an individual?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\asyncio\\runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# ── Quick smoke-test: call the agent directly ─────────────────────\n",
    "#\n",
    "# Jupyter already runs inside an event loop — use `await` directly\n",
    "# rather than asyncio.run(), which raises RuntimeError in a notebook.\n",
    "\n",
    "answer = await agent.run(\"What is the annual deductible for an individual?\")\n",
    "print(\"Q: What is the annual deductible for an individual?\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ebb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Wrap the agent as an A2A server ──────────────────────────────\n",
    "#\n",
    "# Steps (same as Lesson 06 but using MAF instead of QAAgent):\n",
    "#   1. Implement AgentExecutor.execute()  — calls agent.run()\n",
    "#   2. Build AgentCard                    — describes the agent\n",
    "#   3. Wire DefaultRequestHandler         — task lifecycle management\n",
    "#   4. Build A2AStarletteApplication      — ASGI server\n",
    "\n",
    "from a2a.server.agent_execution import AgentExecutor, RequestContext\n",
    "from a2a.server.apps import A2AStarletteApplication\n",
    "from a2a.server.events import EventQueue\n",
    "from a2a.server.request_handlers import DefaultRequestHandler\n",
    "from a2a.server.tasks import InMemoryTaskStore\n",
    "from a2a.types import AgentCapabilities, AgentCard, AgentSkill\n",
    "from a2a.utils import new_agent_text_message\n",
    "from pydantic import AnyHttpUrl\n",
    "\n",
    "\n",
    "class PolicyQAExecutor(AgentExecutor):\n",
    "    \"\"\"Bridges the A2A protocol to the MAF agent.\"\"\"\n",
    "\n",
    "    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:\n",
    "        question = context.get_user_input()\n",
    "        answer = await agent.run(question)\n",
    "        await event_queue.enqueue_event(new_agent_text_message(str(answer)))\n",
    "\n",
    "    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"PolicyQAAgent\",\n",
    "    description=\"Insurance policy Q&A — Microsoft Agent Framework + GitHub Phi-4\",\n",
    "    url=AnyHttpUrl(f\"http://localhost:{SERVER_PORT}/\"),\n",
    "    version=\"1.0.0\",\n",
    "    capabilities=AgentCapabilities(streaming=False),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"policy_qa\",\n",
    "            name=\"Policy Q&A\",\n",
    "            description=\"Answer questions about the Bright Shield insurance policy\",\n",
    "            tags=[\"insurance\", \"qa\"],\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "handler = DefaultRequestHandler(\n",
    "    agent_executor=PolicyQAExecutor(),\n",
    "    task_store=InMemoryTaskStore(),\n",
    ")\n",
    "\n",
    "server_app = A2AStarletteApplication(agent_card=agent_card, http_handler=handler)\n",
    "app = server_app.build()\n",
    "\n",
    "print(\"✓ A2A app built\")\n",
    "print(f\"  Agent card : http://localhost:{SERVER_PORT}/.well-known/agent-card.json\")\n",
    "print(f\"  JSON-RPC   : POST http://localhost:{SERVER_PORT}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Start the server in a background thread ──────────────────────\n",
    "#\n",
    "# The server runs in a daemon thread so the notebook stays interactive.\n",
    "# Open client.ipynb to send A2A requests to this server.\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "\n",
    "def _run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=SERVER_PORT, log_level=\"warning\")\n",
    "\n",
    "\n",
    "server_thread = threading.Thread(target=_run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "import time\n",
    "\n",
    "time.sleep(1.0)  # give uvicorn a moment to bind\n",
    "\n",
    "import httpx\n",
    "\n",
    "resp = httpx.get(\n",
    "    f\"http://localhost:{SERVER_PORT}/.well-known/agent-card.json\", timeout=5\n",
    ")\n",
    "card = resp.json()\n",
    "print(f\"✓ Server live — status {resp.status_code}\")\n",
    "print(f\"  Name        : {card.get('name')}\")\n",
    "print(f\"  Description : {card.get('description')}\")\n",
    "print()\n",
    "print(\"Now open client.ipynb to send A2A requests →\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
