{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66613c50",
   "metadata": {},
   "source": [
    "# Lesson 06 — Wrapping Agents as A2A Servers\n",
    "\n",
    "Transform the standalone QAAgent into a fully **A2A-compliant server**.\n",
    "Supports **GitHub Models** (free, cloud) and **AI Toolkit LocalFoundry** (local, no token required).\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Implement the `AgentExecutor` interface from the A2A Python SDK\n",
    "- Define an Agent Card with skills and capabilities\n",
    "- Wire up `DefaultRequestHandler` + `A2AStarletteApplication`\n",
    "- Test with curl commands\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Lesson 05 completed (QAAgent class)\n",
    "\n",
    "**GitHub Models** (default):\n",
    "\n",
    "- `GITHUB_TOKEN` environment variable set in `.env`\n",
    "\n",
    "**AI Toolkit LocalFoundry** (alternative — no token needed):\n",
    "\n",
    "- VS Code AI Toolkit extension with a model running on port 5272\n",
    "\n",
    "> **A2A SDK docs:** [pypi.org/project/a2a-sdk](https://pypi.org/project/a2a-sdk/)\n",
    ">\n",
    "> **A2A Samples:** [github.com/a2aproject/a2a-samples](https://github.com/a2aproject/a2a-samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f82e3",
   "metadata": {},
   "source": [
    "## Step 1 — Install the A2A SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f54ab41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T21:33:55.422085Z",
     "iopub.status.busy": "2026-02-27T21:33:55.421084Z",
     "iopub.status.idle": "2026-02-27T21:33:55.487001Z",
     "shell.execute_reply": "2026-02-27T21:33:55.487001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Dependencies ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ── Dependencies ──────────────────────────────────────────────────────────────\n",
    "# a2a-sdk[http-server] → A2A Python SDK + Starlette ASGI server + uvicorn\n",
    "# openai               → OpenAI-compatible SDK (GitHub Models / LocalFoundry)\n",
    "# python-dotenv        → Loads GITHUB_TOKEN from the .env file automatically\n",
    "#\n",
    "# If you have the venv active (a2a-examples kernel selected) these are already\n",
    "# installed. Run this cell anyway to ensure the kernel is up to date.\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "%pip install \"a2a-sdk[http-server]\" openai python-dotenv --quiet\n",
    "print(\"Dependencies ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c499a614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T21:33:55.489015Z",
     "iopub.status.busy": "2026-02-27T21:33:55.488040Z",
     "iopub.status.idle": "2026-02-27T21:33:55.496281Z",
     "shell.execute_reply": "2026-02-27T21:33:55.496281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env from: y:\\.sources\\localm-tuts\\a2a\\_examples\\.env\n",
      "GITHUB_TOKEN is set (github_p...)\n",
      "Provider  : github\n",
      "Endpoint  : https://models.inference.ai.azure.com\n",
      "Model     : Phi-4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# ── Load secrets from .env ─────────────────────────────────────────────────\n",
    "# find_dotenv() searches upward from this notebook's working directory.\n",
    "# It will find _examples/.env which contains GITHUB_TOKEN and optionally\n",
    "# LOCALFOUNDRY_ENDPOINT / LOCALFOUNDRY_MODEL.\n",
    "#\n",
    "# To set up your .env:\n",
    "#   cp _examples/.env.example _examples/.env\n",
    "#   # then edit .env and add:  GITHUB_TOKEN=ghp_your_token_here\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "env_path = find_dotenv(raise_error_if_not_found=False)\n",
    "if env_path:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"Loaded .env from: {env_path}\")\n",
    "else:\n",
    "    print(\"NOTE: .env not found — set GITHUB_TOKEN manually if needed\")\n",
    "    # os.environ[\"GITHUB_TOKEN\"] = \"ghp_your_token_here\"\n",
    "\n",
    "# ── Model provider ──────────────────────────────────────────────────────────\n",
    "# \"github\"       — GitHub Models  (free, needs GITHUB_TOKEN in .env)\n",
    "#                  https://github.com/settings/tokens\n",
    "# \"localfoundry\" — AI Toolkit LocalFoundry  (local, no token needed)\n",
    "#                  VS Code AI Toolkit → Models → Load a model → Run\n",
    "#                  Override endpoint/model via LOCALFOUNDRY_ENDPOINT /\n",
    "#                  LOCALFOUNDRY_MODEL in .env if you changed the port or model.\n",
    "PROVIDER = \"github\"  # ← change to \"localfoundry\" to use a local model\n",
    "\n",
    "if PROVIDER == \"github\":\n",
    "    token = os.environ.get(\"GITHUB_TOKEN\", \"\")\n",
    "    if token:\n",
    "        print(f\"GITHUB_TOKEN is set ({token[:8]}...)\")\n",
    "    else:\n",
    "        print(\"ERROR: GITHUB_TOKEN is NOT set — cells below will fail until you set it\")\n",
    "    ENDPOINT = \"https://models.inference.ai.azure.com\"\n",
    "    API_KEY = token\n",
    "    MODEL = \"Phi-4\"\n",
    "\n",
    "elif PROVIDER == \"localfoundry\":\n",
    "    ENDPOINT = os.environ.get(\"LOCALFOUNDRY_ENDPOINT\", \"http://localhost:5272/v1/\")\n",
    "    API_KEY = \"unused\"  # LocalFoundry ignores the key\n",
    "    MODEL = os.environ.get(\"LOCALFOUNDRY_MODEL\", \"qwen2.5-0.5b-instruct-generic-gpu:4\")\n",
    "    print(f\"NOTE: AI Toolkit LocalFoundry — ensure a model is running at {ENDPOINT}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown PROVIDER: {PROVIDER!r}\")\n",
    "\n",
    "print(f\"Provider  : {PROVIDER}\")\n",
    "print(f\"Endpoint  : {ENDPOINT}\")\n",
    "print(f\"Model     : {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e2c89",
   "metadata": {},
   "source": [
    "## Step 2 — Recreate the QAAgent\n",
    "\n",
    "We redefine `QAAgent` inline so this notebook is self-contained.\n",
    "In production, you'd import it from a shared module.\n",
    "\n",
    "> This is the same class from Lesson 05. The `server.py` file imports\n",
    "> it from the Lesson 05 `qa_agent.py` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5336f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T21:33:55.498658Z",
     "iopub.status.busy": "2026-02-27T21:33:55.498658Z",
     "iopub.status.idle": "2026-02-27T21:33:55.972649Z",
     "shell.execute_reply": "2026-02-27T21:33:55.972649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAAgent defined.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful insurance policy assistant.\n",
    "Use the following policy document to answer questions accurately.\n",
    "If the answer is not in the document, say so clearly.\n",
    "\n",
    "--- POLICY DOCUMENT ---\n",
    "{policy_text}\n",
    "--- END DOCUMENT ---\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_knowledge(path: str) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "class QAAgent:\n",
    "    def __init__(self, knowledge_path: str):\n",
    "        self.client = AsyncOpenAI(\n",
    "            base_url=ENDPOINT,\n",
    "            api_key=API_KEY,\n",
    "        )\n",
    "        self.model = MODEL\n",
    "        self.knowledge = load_knowledge(knowledge_path)\n",
    "        self.system_prompt = SYSTEM_PROMPT.format(policy_text=self.knowledge)\n",
    "\n",
    "    async def query(self, question: str) -> str:\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=2048,  # required by GitHub Models free-tier (4 000 out limit)\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(\"QAAgent defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3330eb3",
   "metadata": {},
   "source": [
    "## Step 3 — Implement the AgentExecutor\n",
    "\n",
    "The `AgentExecutor` is the bridge between the A2A protocol and your agent logic:\n",
    "\n",
    "1. `context.get_user_input()` — extracts text from the incoming message\n",
    "2. Call your agent's `query()` method\n",
    "3. `event_queue.enqueue_event(new_agent_text_message(...))` — emit the response\n",
    "\n",
    "**Key imports:**\n",
    "\n",
    "```python\n",
    "from a2a.server.agent_execution import AgentExecutor, RequestContext\n",
    "from a2a.server.events import EventQueue\n",
    "from a2a.utils import new_agent_text_message\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dee812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T21:33:55.974656Z",
     "iopub.status.busy": "2026-02-27T21:33:55.973653Z",
     "iopub.status.idle": "2026-02-27T21:33:56.049964Z",
     "shell.execute_reply": "2026-02-27T21:33:56.049964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAAgentExecutor defined.\n"
     ]
    }
   ],
   "source": [
    "from a2a.server.agent_execution import AgentExecutor, RequestContext\n",
    "from a2a.server.events import EventQueue\n",
    "from a2a.utils import new_agent_text_message\n",
    "\n",
    "\n",
    "class QAAgentExecutor(AgentExecutor):\n",
    "    \"\"\"Wraps QAAgent with the A2A AgentExecutor interface.\"\"\"\n",
    "\n",
    "    def __init__(self, knowledge_path: str = \"data/insurance_policy.txt\"):\n",
    "        self.agent = QAAgent(knowledge_path)\n",
    "\n",
    "    async def execute(\n",
    "        self,\n",
    "        context: RequestContext,\n",
    "        event_queue: EventQueue,\n",
    "    ) -> None:\n",
    "        # 1. Extract user message text\n",
    "        question = context.get_user_input()\n",
    "\n",
    "        # 2. Call the QA agent\n",
    "        answer = await self.agent.query(question)\n",
    "\n",
    "        # 3. Emit the response as an A2A event\n",
    "        await event_queue.enqueue_event(new_agent_text_message(answer))\n",
    "\n",
    "    async def cancel(\n",
    "        self,\n",
    "        context: RequestContext,\n",
    "        event_queue: EventQueue,\n",
    "    ) -> None:\n",
    "        raise Exception(\"cancel not supported\")\n",
    "\n",
    "\n",
    "print(\"QAAgentExecutor defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded61bf",
   "metadata": {},
   "source": [
    "## Step 4 — Define the Agent Card\n",
    "\n",
    "The Agent Card is served at `/.well-known/agent.json`. It tells clients:\n",
    "\n",
    "- What the agent can do (skills)\n",
    "- What capabilities it supports (streaming, push notifications)\n",
    "- What content types it accepts/produces\n",
    "\n",
    "Think of it as a **capability manifest** for your agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0aa8c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T21:33:56.051969Z",
     "iopub.status.busy": "2026-02-27T21:33:56.051969Z",
     "iopub.status.idle": "2026-02-27T21:33:56.066278Z",
     "shell.execute_reply": "2026-02-27T21:33:56.066278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"capabilities\": {\n",
      "    \"streaming\": true\n",
      "  },\n",
      "  \"defaultInputModes\": [\n",
      "    \"text\"\n",
      "  ],\n",
      "  \"defaultOutputModes\": [\n",
      "    \"text\"\n",
      "  ],\n",
      "  \"description\": \"Answers questions about insurance policies using Phi-4 via github\",\n",
      "  \"name\": \"QAAgent\",\n",
      "  \"preferredTransport\": \"JSONRPC\",\n",
      "  \"protocolVersion\": \"0.3.0\",\n",
      "  \"skills\": [\n",
      "    {\n",
      "      \"description\": \"Answer questions about insurance policy documents\",\n",
      "      \"examples\": [\n",
      "        \"What is the deductible for the Standard plan?\",\n",
      "        \"Are cosmetic procedures covered?\",\n",
      "        \"How do I file a claim?\"\n",
      "      ],\n",
      "      \"id\": \"policy-qa\",\n",
      "      \"name\": \"Policy Question Answering\",\n",
      "      \"tags\": [\n",
      "        \"qa\",\n",
      "        \"insurance\",\n",
      "        \"policy\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"url\": \"http://localhost:10001/\",\n",
      "  \"version\": \"1.0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from a2a.types import AgentCapabilities, AgentCard, AgentSkill\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"QAAgent\",\n",
    "    description=f\"Answers questions about insurance policies using {MODEL} via {PROVIDER}\",\n",
    "    url=\"http://localhost:10001/\",\n",
    "    version=\"1.0.0\",\n",
    "    capabilities=AgentCapabilities(streaming=True),\n",
    "    default_input_modes=[\"text\"],\n",
    "    default_output_modes=[\"text\"],\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"policy-qa\",\n",
    "            name=\"Policy Question Answering\",\n",
    "            description=\"Answer questions about insurance policy documents\",\n",
    "            tags=[\"qa\", \"insurance\", \"policy\"],\n",
    "            examples=[\n",
    "                \"What is the deductible for the Standard plan?\",\n",
    "                \"Are cosmetic procedures covered?\",\n",
    "                \"How do I file a claim?\",\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Preview the Agent Card as JSON\n",
    "print(agent_card.model_dump_json(indent=2, exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150ca8a",
   "metadata": {},
   "source": [
    "## Step 5 — Wire Up the Server\n",
    "\n",
    "Three components wired together:\n",
    "\n",
    "1. **`DefaultRequestHandler`** — routes requests to the executor, manages task state\n",
    "2. **`A2AStarletteApplication`** — builds the ASGI app (Starlette-based)\n",
    "3. **`uvicorn`** — serves the app\n",
    "\n",
    "```python\n",
    "from a2a.server.apps import A2AStarletteApplication\n",
    "from a2a.server.request_handlers import DefaultRequestHandler\n",
    "from a2a.server.tasks import InMemoryTaskStore\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84bc17c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T21:33:56.068282Z",
     "iopub.status.busy": "2026-02-27T21:33:56.068282Z",
     "iopub.status.idle": "2026-02-27T21:33:56.563644Z",
     "shell.execute_reply": "2026-02-27T21:33:56.562637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI app built — type: Starlette\n",
      "Agent Card URL: http://localhost:10001/.well-known/agent.json\n"
     ]
    }
   ],
   "source": [
    "from a2a.server.apps import A2AStarletteApplication\n",
    "from a2a.server.request_handlers import DefaultRequestHandler\n",
    "from a2a.server.tasks import InMemoryTaskStore\n",
    "\n",
    "request_handler = DefaultRequestHandler(\n",
    "    agent_executor=QAAgentExecutor(),\n",
    "    task_store=InMemoryTaskStore(),\n",
    ")\n",
    "\n",
    "server = A2AStarletteApplication(\n",
    "    agent_card=agent_card,\n",
    "    http_handler=request_handler,\n",
    ")\n",
    "\n",
    "app = server.build()\n",
    "print(f\"ASGI app built — type: {type(app).__name__}\")\n",
    "print(f\"Agent Card URL: {agent_card.url}.well-known/agent.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0caf0",
   "metadata": {},
   "source": [
    "## Step 6 — Run the Server\n",
    "\n",
    "> **Note:** Jupyter already runs an asyncio event loop, so `uvicorn.run()` raises\n",
    "> `RuntimeError: Runner.run() cannot be called from a running event loop`.\n",
    "> Use `await server.serve()` instead — Jupyter supports top-level `await`.\n",
    "\n",
    "Running the server **blocks this notebook**. To keep the notebook interactive,\n",
    "run the server from a terminal instead:\n",
    "\n",
    "```bash\n",
    "cd src\n",
    "python server.py\n",
    "```\n",
    "\n",
    "Or run the cell below (it will block until you interrupt the kernel):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ab810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T21:33:56.565643Z",
     "iopub.status.busy": "2026-02-27T21:33:56.564645Z",
     "iopub.status.idle": "2026-02-27T21:33:56.567689Z",
     "shell.execute_reply": "2026-02-27T21:33:56.567689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [30540]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:10001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting QAAgent A2A Server on http://localhost:10001\n",
      "Agent Card: http://localhost:10001/.well-known/agent.json\n",
      "Press Ctrl+C or interrupt kernel to stop\n",
      "INFO:     127.0.0.1:63192 - \"GET /.well-known/agent-card.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63194 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63194 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63194 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63194 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63194 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63194 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63194 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63194 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to run the server (this will block until you interrupt the kernel):\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "config = uvicorn.Config(app, host=\"0.0.0.0\", port=10001)\n",
    "server_instance = uvicorn.Server(config)\n",
    "\n",
    "print(\"Starting QAAgent A2A Server on http://localhost:10001\")\n",
    "print(\"Agent Card: http://localhost:10001/.well-known/agent.json\")\n",
    "print(\"Press Ctrl+C or interrupt kernel to stop\")\n",
    "await server_instance.serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af224006",
   "metadata": {},
   "source": [
    "## Step 7 — Test with curl\n",
    "\n",
    "With the server running (from terminal), test it:\n",
    "\n",
    "### Fetch Agent Card\n",
    "\n",
    "```bash\n",
    "curl http://localhost:10001/.well-known/agent.json | python -m json.tool\n",
    "```\n",
    "\n",
    "### Send a Test Question\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:10001 \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"id\": \"test-1\",\n",
    "    \"method\": \"message/send\",\n",
    "    \"params\": {\n",
    "      \"message\": {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"kind\": \"text\", \"text\": \"What is the deductible?\"}],\n",
    "        \"messageId\": \"msg-001\"\n",
    "      }\n",
    "    }\n",
    "  }'\n",
    "```\n",
    "\n",
    "You should see a JSON-RPC response with a Task containing the agent's answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135aa1eb",
   "metadata": {},
   "source": [
    "## Port Convention\n",
    "\n",
    "Each agent in this course runs on a dedicated port:\n",
    "\n",
    "| Port  | Agent             | Lesson |\n",
    "| ----- | ----------------- | ------ |\n",
    "| 10001 | QAAgent           | 5-7    |\n",
    "| 10002 | ResearchAgent     | 9      |\n",
    "| 10003 | CodeAgent         | 10     |\n",
    "| 10004 | PlannerAgent      | 11     |\n",
    "| 10005 | TaskAgent         | 12     |\n",
    "| 10006 | AssistantAgent    | 13     |\n",
    "| 10007 | CopilotAgent      | 14     |\n",
    "| 10008 | OrchestratorAgent | 15     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302a6de",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Keep the server running!** In Lesson 7, you'll build a client that:\n",
    "\n",
    "- Discovers this agent via its Agent Card\n",
    "- Sends blocking and streaming requests\n",
    "- Parses Tasks, Artifacts, and Messages\n",
    "\n",
    "→ Continue to [Lesson 07 — A2A Client Fundamentals](../07-a2a-client/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
