{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2b83df",
   "metadata": {},
   "source": [
    "# Lesson 09 — Google ADK: A2A Server\n",
    "\n",
    "This notebook shows how to build an **A2A server** using **Google Agent Development Kit** with your choice of model provider.\n",
    "\n",
    "## What we build\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    Tool[\"FunctionTool\\nget_policy_info\"]\n",
    "    LLM[\"LiteLlm\\nGitHub Phi-4 or LocalFoundry\"]\n",
    "    Tool --> Agent[\"LlmAgent\\nname · instruction · tools\"]\n",
    "    LLM  --> Agent\n",
    "    Agent -->|\"to_a2a() one-liner\"| Server[\"Starlette ASGI app\\nport 10091\"]\n",
    "    Server -->|\"A2A JSON-RPC\"| Client[\"Any A2A Client\"]\n",
    "```\n",
    "\n",
    "**Same use case as Lesson 05–08:** insurance policy Q&A assistant.\n",
    "\n",
    "### Model provider options\n",
    "\n",
    "| Provider                    | Variable value   | What you need                                                                 |\n",
    "| --------------------------- | ---------------- | ----------------------------------------------------------------------------- |\n",
    "| **GitHub Models**           | `\"github\"`       | `GITHUB_TOKEN` in `.env` — [get one free](https://github.com/settings/tokens) |\n",
    "| **AI Toolkit LocalFoundry** | `\"localfoundry\"` | VS Code AI Toolkit extension, model loaded on port 5272                       |\n",
    "\n",
    "Set `PROVIDER` in cell 2 before running.\n",
    "\n",
    "Port used: **10091** (notebooks) · Full lesson script uses 10002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports & environment setup ──────────────────────────────────\n",
    "import os\n",
    "import threading\n",
    "import warnings\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(raise_error_if_not_found=False))\n",
    "\n",
    "# ── Model provider ────────────────────────────────────────────────\n",
    "# \"github\"       — GitHub Models  (free, needs GITHUB_TOKEN in .env)\n",
    "#                  https://github.com/settings/tokens\n",
    "# \"localfoundry\" — AI Toolkit LocalFoundry  (local, no token needed)\n",
    "#                  VS Code AI Toolkit → Models → Load a model → Run\n",
    "PROVIDER = \"github\"  # ← change to \"localfoundry\" to use a local model\n",
    "\n",
    "if PROVIDER == \"github\":\n",
    "    os.environ[\"OPENAI_API_BASE\"] = \"https://models.inference.ai.azure.com\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"GITHUB_TOKEN\", \"\")\n",
    "    LITELLM_MODEL = \"openai/Phi-4\"\n",
    "    if not os.environ[\"OPENAI_API_KEY\"]:\n",
    "        raise EnvironmentError(\"Set GITHUB_TOKEN in your .env or environment first\")\n",
    "\n",
    "elif PROVIDER == \"localfoundry\":\n",
    "    os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:5272/v1/\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"unused\"  # LocalFoundry ignores the key\n",
    "    LITELLM_MODEL = \"openai/Phi-4-mini-instruct\"  # ← change to your loaded model ID\n",
    "    print(\"ℹ  AI Toolkit LocalFoundry — ensure a model is running on port 5272\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown PROVIDER: {PROVIDER!r}\")\n",
    "\n",
    "SERVER_PORT = 10091\n",
    "\n",
    "print(f\"✓ Provider  : {PROVIDER}\")\n",
    "print(f\"✓ API base  : {os.environ['OPENAI_API_BASE']}\")\n",
    "print(f\"✓ Model     : {LITELLM_MODEL}\")\n",
    "print(f\"✓ Port      : {SERVER_PORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5338a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Knowledge base (inline) ───────────────────────────────────────\n",
    "POLICY = \"\"\"\n",
    "BRIGHT SHIELD INSURANCE — POLICY SUMMARY\n",
    "\n",
    "Coverage Period:  January 1 – December 31 2025\n",
    "Policy Number:    BS-2025-DEMO\n",
    "\n",
    "DEDUCTIBLES\n",
    "  Annual deductible (individual) : $500\n",
    "  Annual deductible (family)     : $1,000\n",
    "  Out-of-pocket maximum          : $3,500 / person\n",
    "\n",
    "PREMIUMS\n",
    "  Monthly premium (individual)   : $120\n",
    "  Monthly premium (family)       : $340\n",
    "\n",
    "COVERAGE\n",
    "  Liability                      : $100,000 per incident\n",
    "  Collision                      : Covered (ACV after deductible)\n",
    "  Comprehensive                  : Covered (ACV after deductible)\n",
    "  Rental car reimbursement       : Up to $35/day, 30-day maximum\n",
    "  Emergency roadside assistance  : Included (unlimited calls)\n",
    "  Medical payments               : $5,000 per person\n",
    "\n",
    "EXCLUSIONS\n",
    "  Intentional damage, racing, commercial use are NOT covered.\n",
    "\n",
    "CLAIMS\n",
    "  File within 30 days of incident.\n",
    "  Contact: claims@brightshield.example  |  1-800-555-0199\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Policy document loaded ({len(POLICY)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build the Google ADK agent ────────────────────────────────────\n",
    "#\n",
    "# Pattern:\n",
    "#   1. FunctionTool     — wraps a plain Python function as a tool\n",
    "#   2. LiteLlm          — model provider (endpoint set by PROVIDER)\n",
    "#   3. LlmAgent         — connects model + tools + instructions\n",
    "\n",
    "from google.adk.agents import LlmAgent  # type: ignore[import-untyped]\n",
    "from google.adk.models.lite_llm import LiteLlm  # type: ignore[import-untyped]\n",
    "from google.adk.tools import FunctionTool  # type: ignore[import-untyped]\n",
    "\n",
    "\n",
    "def get_policy_info(topic: str) -> str:\n",
    "    \"\"\"Return the insurance policy document.  topic is ignored.\"\"\"\n",
    "    return POLICY\n",
    "\n",
    "\n",
    "model = LiteLlm(model=LITELLM_MODEL)\n",
    "\n",
    "agent = LlmAgent(\n",
    "    model=model,\n",
    "    name=\"PolicyQAAgent\",\n",
    "    description=\"Insurance policy Q&A powered by Google ADK\",\n",
    "    instruction=(\n",
    "        \"You are a helpful insurance assistant for Bright Shield Insurance. \"\n",
    "        \"Use the get_policy_info tool to look up policy details and answer \"\n",
    "        \"questions accurately. Keep answers concise.\"\n",
    "    ),\n",
    "    tools=[FunctionTool(get_policy_info)],\n",
    ")\n",
    "\n",
    "print(f\"✓ Agent created : {agent.name}\")\n",
    "print(f\"  Provider      : {PROVIDER}  ({LITELLM_MODEL})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Wrap with to_a2a() — ADK's one-liner A2A export ───────────────\n",
    "#\n",
    "# to_a2a() emits an EXPERIMENTAL UserWarning; suppress it so the\n",
    "# notebook output stays clean.\n",
    "\n",
    "from a2a.utils.a2a_tools import to_a2a  # type: ignore[import-untyped]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    app = to_a2a(agent, host=\"localhost\", port=SERVER_PORT)\n",
    "\n",
    "print(\"✓ A2A app built via to_a2a()\")\n",
    "print(f\"  Agent card : http://localhost:{SERVER_PORT}/.well-known/agent-card.json\")\n",
    "print(f\"  JSON-RPC   : POST http://localhost:{SERVER_PORT}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c734db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Start the server in a background thread ──────────────────────\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "\n",
    "def _run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=SERVER_PORT, log_level=\"warning\")\n",
    "\n",
    "\n",
    "server_thread = threading.Thread(target=_run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "import time\n",
    "\n",
    "time.sleep(1.5)  # give uvicorn a moment to bind\n",
    "\n",
    "import httpx\n",
    "\n",
    "resp = httpx.get(\n",
    "    f\"http://localhost:{SERVER_PORT}/.well-known/agent-card.json\", timeout=5\n",
    ")\n",
    "card = resp.json()\n",
    "print(f\"✓ Server live — status {resp.status_code}\")\n",
    "print(f\"  Name        : {card.get('name')}\")\n",
    "print(f\"  Description : {card.get('description')}\")\n",
    "print()\n",
    "print(\"Now open client.ipynb to send A2A requests →\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
