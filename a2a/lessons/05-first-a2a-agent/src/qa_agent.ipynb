{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350c7056",
   "metadata": {},
   "source": [
    "# Lesson 05 ‚Äî Building Your First A2A Agent\n",
    "\n",
    "Build a standalone QA agent powered by **GitHub Phi-4** that answers questions about insurance policies.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Configure the OpenAI-compatible API for GitHub Models\n",
    "- Load domain knowledge and inject it into a system prompt\n",
    "- Build a `QAAgent` async class\n",
    "- Test the agent standalone before adding A2A protocol layers\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- GitHub account with a [Personal Access Token](https://github.com/settings/tokens) (no special scopes)\n",
    "- `GITHUB_TOKEN` environment variable set\n",
    "- Python 3.10+\n",
    "\n",
    "> **GitHub Models docs:** [docs.github.com/en/github-models](https://docs.github.com/en/github-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125e711",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äî Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Dependencies ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# openai        ‚Üí OpenAI-compatible SDK (used to call GitHub Models / Phi-4)\n",
    "# python-dotenv ‚Üí Loads GITHUB_TOKEN from the .env file automatically\n",
    "#\n",
    "# If you have the venv active (a2a-examples kernel selected) these are already\n",
    "# installed. Run this cell anyway to ensure the kernel is up to date.\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "%pip install openai python-dotenv --quiet\n",
    "print(\"‚úÖ Dependencies ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ce57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ Load secrets from .env ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# find_dotenv() searches upward from this notebook's working directory.\n",
    "# It will find _examples/.env which contains GITHUB_TOKEN.\n",
    "#\n",
    "# To set up your .env:\n",
    "#   cp _examples/.env.example _examples/.env\n",
    "#   # then edit .env and add:  GITHUB_TOKEN=ghp_your_token_here\n",
    "#\n",
    "# Get a free GitHub PAT (no special scopes needed):\n",
    "#   https://github.com/settings/tokens\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "env_path = find_dotenv(raise_error_if_not_found=False)\n",
    "if env_path:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"‚úÖ Loaded .env from: {env_path}\")\n",
    "else:\n",
    "    print(\"‚ö†  .env not found ‚Äî set GITHUB_TOKEN manually below if needed\")\n",
    "    # Uncomment and paste your token if .env is not available:\n",
    "    # os.environ[\"GITHUB_TOKEN\"] = \"ghp_your_token_here\"\n",
    "\n",
    "token = os.environ.get(\"GITHUB_TOKEN\", \"\")\n",
    "if token:\n",
    "    print(f\"‚úÖ GITHUB_TOKEN is set ({token[:8]}...)\")\n",
    "else:\n",
    "    print(\"‚ùå GITHUB_TOKEN is NOT set ‚Äî cells below will fail until you set it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f8f13",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äî Configure the Model Client\n",
    "\n",
    "GitHub Models provides an **OpenAI-compatible API**. We use the standard `openai` package,\n",
    "pointed at `https://models.inference.ai.azure.com` instead of OpenAI's endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Load .env file if present (optional ‚Äî you can also export GITHUB_TOKEN directly)\n",
    "load_dotenv()\n",
    "\n",
    "# Verify token is set\n",
    "assert os.environ.get(\"GITHUB_TOKEN\"), \"Set GITHUB_TOKEN environment variable first!\"\n",
    "\n",
    "# Create the async OpenAI client pointing at GitHub Models\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://models.inference.ai.azure.com\",\n",
    "    api_key=os.environ[\"GITHUB_TOKEN\"],\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Client configured ‚Äî base_url: {client.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf470ceb",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äî Quick Model Test\n",
    "\n",
    "Before building the agent, verify that the model connection works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8015197",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await client.chat.completions.create(\n",
    "    model=\"Phi-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Answer: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6338f9f",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äî Load Domain Knowledge\n",
    "\n",
    "The agent needs a knowledge base to answer domain-specific questions.\n",
    "We load an insurance policy document and inject it into the system prompt.\n",
    "\n",
    "This is **RAG-lite** ‚Äî simple and effective for bounded domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful insurance policy assistant.\n",
    "Use the following policy document to answer questions accurately.\n",
    "If the answer is not in the document, say so clearly.\n",
    "Always cite the relevant section when possible.\n",
    "\n",
    "--- POLICY DOCUMENT ---\n",
    "{policy_text}\n",
    "--- END DOCUMENT ---\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_knowledge(path: str) -> str:\n",
    "    \"\"\"Load a knowledge document from disk.\"\"\"\n",
    "    return Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Load the insurance policy\n",
    "knowledge = load_knowledge(\"data/insurance_policy.txt\")\n",
    "system_prompt = SYSTEM_PROMPT.format(policy_text=knowledge)\n",
    "\n",
    "print(f\"üìÑ Loaded {len(knowledge)} characters of domain knowledge\")\n",
    "print(f\"üìù System prompt: {len(system_prompt)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be69ff92",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äî Build the QAAgent Class\n",
    "\n",
    "The agent encapsulates:\n",
    "\n",
    "- Model client (AsyncOpenAI ‚Üí GitHub Phi-4)\n",
    "- Domain knowledge (loaded from file)\n",
    "- System prompt (template with injected knowledge)\n",
    "\n",
    "**Key design decisions:**\n",
    "| Decision | Choice | Rationale |\n",
    "|---|---|---|\n",
    "| Async interface | `async def query()` | A2A servers are async ‚Äî start async from day one |\n",
    "| Class pattern | `QAAgent` class | Clean separation for AgentExecutor wrapping (Lesson 6) |\n",
    "| Low temperature | `0.2` | Factual Q&A needs deterministic responses |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373adef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAAgent:\n",
    "    \"\"\"Question-answering agent backed by GitHub Phi-4.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        knowledge_path: str,\n",
    "        model: str = \"Phi-4\",\n",
    "        temperature: float = 0.2,\n",
    "    ):\n",
    "        self.client = AsyncOpenAI(\n",
    "            base_url=\"https://models.inference.ai.azure.com\",\n",
    "            api_key=os.environ[\"GITHUB_TOKEN\"],\n",
    "        )\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.knowledge = load_knowledge(knowledge_path)\n",
    "        self.system_prompt = SYSTEM_PROMPT.format(\n",
    "            policy_text=self.knowledge\n",
    "        )\n",
    "\n",
    "    async def query(self, question: str) -> str:\n",
    "        \"\"\"Send a question to Phi-4 and return the answer.\"\"\"\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(\"‚úÖ QAAgent class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c173e2",
   "metadata": {},
   "source": [
    "## Step 6 ‚Äî Test the Agent\n",
    "\n",
    "**Always test standalone before wrapping in A2A.** This verifies:\n",
    "\n",
    "- Model connectivity (GitHub Models endpoint)\n",
    "- Knowledge injection (system prompt)\n",
    "- Response quality (grounded, factual answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee45cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = QAAgent(\"data/insurance_policy.txt\")\n",
    "print(f\"‚úÖ Agent created with {len(agent.knowledge)} chars of knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5775b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 1: Specific fact\n",
    "answer = await agent.query(\"What is the deductible for the Standard plan?\")\n",
    "print(f\"‚ùì What is the deductible for the Standard plan?\\n\")\n",
    "print(f\"üí¨ {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f12c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 2: Coverage question\n",
    "answer = await agent.query(\"Are cosmetic procedures covered?\")\n",
    "print(f\"‚ùì Are cosmetic procedures covered?\\n\")\n",
    "print(f\"üí¨ {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 3: Out-of-scope question (should say \"not in document\")\n",
    "answer = await agent.query(\"What is the capital of France?\")\n",
    "print(f\"‚ùì What is the capital of France?\\n\")\n",
    "print(f\"üí¨ {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7869c12",
   "metadata": {},
   "source": [
    "## Step 7 ‚Äî Experiment\n",
    "\n",
    "Try these variations:\n",
    "\n",
    "- Change the `temperature` (0.0 for max determinism, 0.8 for more creativity)\n",
    "- Ask follow-up questions about coverage limits, claims process, etc.\n",
    "- Ask questions not covered by the document ‚Äî the agent should say so clearly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question!\n",
    "your_question = \"How much is the monthly premium?\"\n",
    "\n",
    "answer = await agent.query(your_question)\n",
    "print(f\"‚ùì {your_question}\\n\")\n",
    "print(f\"üí¨ {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70181b55",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This QAAgent is the **foundation** for every A2A agent in this course.\n",
    "\n",
    "- **Lesson 6** ‚Üí Wrap this agent as an A2A server (AgentExecutor + Agent Card)\n",
    "- **Lesson 7** ‚Üí Build a client that discovers and calls the server\n",
    "\n",
    "The async class pattern ‚Äî `QAAgent` with `async def query()` ‚Äî is the core\n",
    "that Lesson 6 wraps with the A2A SDK's `AgentExecutor` interface.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  },
  "kernelspec": {
   "display_name": "A2A Examples (Python 3.11)",
   "language": "python",
   "name": "a2a-examples"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}