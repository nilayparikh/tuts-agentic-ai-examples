# Multi-Agent Loan Approval System — Environment Variables
#
# This lesson uses the SHARED .env at the repo root:
#   _examples/.env
#
# find_dotenv() walks up the directory tree automatically,
# so you do NOT need a local .env in this folder.
# Only create a local .env here to OVERRIDE the shared values.
#
# ── Required variables (set in _examples/.env) ───────────────
# GITHUB_TOKEN             — GitHub PAT for GitHub Models
# AZURE_OPENAI_ENDPOINT    — Azure OpenAI endpoint
# AZURE_AI_API_KEY         — Azure OpenAI API key
# AZURE_AI_MODEL_DEPLOYMENT_NAME — Azure model deployment

# ── Model Provider Selection ─────────────────────────────────
# Values: github | microsoftfoundry | localfoundry
# Default: github (GitHub Models — Phi-4, free tier)
PROVIDER=github

# ── GitHub Models (default) ──────────────────────────────────
# Override the model name (default: Phi-4)
# GITHUB_MODEL=Phi-4

# ── LocalFoundry / AI Toolkit ────────────────────────────────
# VS Code AI Toolkit → Models → Load a model → Run
# LOCALFOUNDRY_ENDPOINT=http://localhost:5272/v1/
# LOCALFOUNDRY_MODEL=qwen2.5-0.5b-instruct-generic-gpu:4

# ── Decision Thresholds ──────────────────────────────────────
AUTO_APPROVE_THRESHOLD=40
AUTO_DECLINE_THRESHOLD=80

# ── OpenTelemetry (optional) ─────────────────────────────────
# Set to enable OTLP export (e.g. Jaeger, Grafana Tempo)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
# OTEL_SERVICE_NAME=loan-approval-orchestrator

# ── Escalation REST API ──────────────────────────────────────
ESCALATION_API_PORT=8080
